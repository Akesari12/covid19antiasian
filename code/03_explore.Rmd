---
title: "03_preprocess_data"
author: "Jae Yeon Kim"
date: "7/2/2020"
output: html_document
---

# Import packages and files 

## Packages

```{r}

pacman::p_load(data.table, # for fast data manipulation
               tidyverse, # for tidyverse 
               ggpubr, # for arranging ggplots   
               ggthemes, # for fancy ggplot themes
               here, # for reproducibility 
               maps, # for US city data
               openintro, # for revert state name abbreviations to their original forms 
               stringr, # for easy regular expression
               lubridate, # for easy time var manipulation
               forcats, # reverse factor order 
               patchwork, # for easy ggarrange
               ggsci) # for pubs 

#devtools::install_github("jaeyk/tidytweetjson", force = TRUE)
# devtools::install_github("jaeyk/makereproducible", force = TRUE)

library(tidytweetjson)
library(makereproducible)
# for publication-friendly theme 
theme_set(theme_pubr())

# custom functions
source(here("functions", "stacked_area_plot.R"))

```

## Files 

- 5,050,042 obs 

```{r}

parsed <- readRDS(here("processed_data", "parsed.rds"))

```

# Wrangle 

## Filter

- Filtered if location field is empty or NA: 3,734,756 obs (73% of the original data)
- I did not use country_code field as it is mostly filled with NAs (97%).

```{r}

sum(is.na(parsed$country_code))/nrow(parsed)

filtered <- parsed %>%
    filter(!(location == "" | is.na(location)))

```

## Select 

```{r}

# Select columns 
selected <- filtered %>% 
    select(created_at, full_text, location) 

```

## Mutate 

### Location 

- `maps::us.cities` database includes the names of the US cities whose population sizes are larger than 40,000. 

- 41% of the Tweets were created by the users located in the US.

```{r}

# Identify US location 

us_cities <- maps::us.cities$name %>% str_replace_all(" [[:alpha:]]*$", "") %>% unique() %>% tolower()

us_states <- maps::us.cities$name %>% stringr::word(-1) %>% unique() %>% abbr2state() %>% tolower()

us_country <- c("United States", "USA", "US", "U.S.A.") %>% tolower()

us_location_list <- c(us_cities, us_states, us_country)

```

```{r}

selected$US <- str_detect(selected$location %>% tolower(), us_location_list %>% paste(collapse = "|")) %>% as.numeric()

sum(selected$US)/nrow(selected)

```

### Filter
```{r}

# Filter US cases

df <- selected %>% filter(US == 1)

# Filter incorrect cases (non-US cases still categorized as US cases)

## Country dict 
countryname_dict <- unique(maps::world.cities[,2])

countryname_dict <- countryname_dict[!countryname_dict %in% c("United States", "USA", "U.S.A.")]

## Filter 
df$non_US <- str_detect(df$location %>% tolower(), countryname_dict %>% tolower() %>% paste(collapse = "|")) %>% as.numeric()

sum(df$non_US)/nrow(df) # 90% would remain after the filtering  

df <- df %>% filter(non_US == 0)

```

### Text 

```{r}

# Create new columns 
df <- df %>%
    mutate(full_text = str_to_lower(full_text),
           wuhan = as.numeric(str_detect(full_text, "wuhan")),
           asian = as.numeric(str_detect(full_text, "asia|asian")),
           chinese = as.numeric(str_detect(full_text, "china|chinese")),
           trump = as.numeric(str_detect(full_text, "trump")))

```

### Date

```{r}
df <- tidytweetjson::add_date(df)

#df[1:10,] %>% select(created_at, date)

fwrite(df, here("processed_data", "US_tweets.csv"))

#df <- fread(here("processed_data", "US_tweets.csv"))

```

# Explore 

## Time-series

### Overall 

```{r}

# Google 
gtrends <- read_csv(here("processed_data", "gtrends.csv"))

gtrends_plot <- gtrends %>%
    rename(Terms = keywords) %>%
    ggplot(aes(x = as.Date(date), y = log(hits),
               col = Terms, 
               linetype = Terms)) +
        geom_line(size = 1) +
        labs(x = "Date",
             y = "Logged count",
             title = "Google Searches on COVID-19",
             subtitle = "In the US",
             source = "Google API")

# Twitter 
## Group by date then visualize 
twitter_plot <- df %>%
    pivot_longer(cols = c("asian", "chinese", "wuhan", "trump"), 
                 names_to = "Terms",
                 values_to = "Hits") %>%
    mutate(Terms = stringr::str_to_title(Terms)) %>%
    filter(Hits == 1) %>%
    group_by(Terms, date) %>%
    count() %>%
    ggplot(aes(x = as.Date(date), y = log(n),
               col = Terms,
               linetype = Terms)) +
        geom_line(size = 1) +
        labs(x = "Date",
             y = "Logged count",
             title = "Tweets on COVID-19",
             subtitle = "In the US",
             source = "Twitter API and Panacealab") 
        

gtrends_plot + twitter_plot

ggsave(here("outputs", "overall_trend.png"), width = 10)

```

### Keywords 

```{r}

# df <- data.table::fread(here("processed_data", "text_ready.csv"))

p1 <- stacked_area_plot(df, wuhan) +
        labs(x = "Date",
             y = "Proportion",
             fill = "Wuhan",
             title = "Wuhan",
             subtitle = "Tweets created by the users located in the US")

p2 <- stacked_area_plot(df, chinese) +
        labs(x = "Date",
             y = "Proportion",
             fill = "Chinese",
             title = "Chinese",
             subtitle = "Tweets created by the users located in the US")

p3 <- stacked_area_plot(df, asian) +
        labs(x = "Date",
             y = "Proportion",
             fill = "Asian",
             title = "Asian",
             subtitle = "Tweets created by the users located in the US")

p4 <- stacked_area_plot(df, trump) +
        labs(x = "Date",
             y = "Proportion",
             fill = "Trump",
             title = "Trump",
             subtitle = "Tweets created by the users located in the US")

(p1 + p2) / (p3 + p4)

ggsave(here("outputs", "stacked_bar_plots.png"),
       height = 10, width = 12)
```

# Export 

```{r}

fwrite(df, here("processed_data", "text_ready.csv"))

```

